{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ResNet_cifar10 import BasicBlock, Bottleneck, MyResNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "def Device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def Load_Data():\n",
    "    NUM_TRAIN = 49000\n",
    "\n",
    "    transform_train = T.Compose([\n",
    "                    T.RandomCrop(size=32,padding=4),\n",
    "                    T.RandomHorizontalFlip(),\n",
    "                    T.ToTensor(),\n",
    "                    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                ])\n",
    "\n",
    "    transform = T.Compose([\n",
    "                    T.ToTensor(),\n",
    "                    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                ])\n",
    "\n",
    "    cifar10_train = dset.CIFAR10('./datasets', train=True, download=True, transform=transform_train)\n",
    "    loader_train = DataLoader(cifar10_train, batch_size=128, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "    cifar10_val = dset.CIFAR10('./datasets', train=True, download=True, transform=transform)\n",
    "    loader_val = DataLoader(cifar10_val, batch_size=128, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "    cifar10_test = dset.CIFAR10('./datasets', train=False, download=True, transform=transform)\n",
    "    loader_test = DataLoader(cifar10_test, batch_size=128)\n",
    "\n",
    "    return loader_train, loader_val, loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "accuracy_train_history = []\n",
    "accuracy_val_history = []\n",
    "device = Device()\n",
    "loader_train, loader_val, loader_test = Load_Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def Check_Accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Accuracy on Validation Set: \")\n",
    "    else:\n",
    "        print(\"Accuracy on Test Set: \")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device, dtype = torch.float32)\n",
    "            y = y.to(device, dtype = torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predict = torch.max(scores, 1)\n",
    "            correct += (predict == y).sum()\n",
    "            total += predict.size(0)\n",
    "        accuracy = correct / total\n",
    "        accuracy_val_history.append(accuracy)\n",
    "        print('{:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def Train(model, optimizer, epochs = 1):\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    model = model.to(device = device)\n",
    "    model.train() # setup for BN\n",
    "    for e in range(epochs):\n",
    "        print('Epoch(%d)' % e)\n",
    "        for t,(x,y) in enumerate(loader_train):\n",
    "            x = x.to(device, dtype = torch.float32)\n",
    "            y = y.to(device, dtype = torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(scores, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            _, predict = torch.max(scores, 1)\n",
    "            correct += (predict == y).sum()\n",
    "            total += predict.size(0)\n",
    "            accuracy_train_history.append(correct/total)\n",
    "\n",
    "            if t % 100 == 0:\n",
    "                print('Loss: %.4f' % loss)\n",
    "                loss_history.append(loss)\n",
    "                Check_Accuracy(loader_val, model)\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Draw():\n",
    "    plt.xlabel('n')\n",
    "    plt.ylabel('loss')\n",
    "    n = np.arange(1, len(loss_history) + 1)\n",
    "    m = np.arange(1, len(accuracy_val_history) + 1)\n",
    "    k = np.arange(1, len(accuracy_train_history) + 1) / 100\n",
    "    plt.plot(n, loss_history, color = 'g')\n",
    "    plt.plot(m, accuracy_val_history, color = 'r')\n",
    "    plt.plot(k, accuracy_train_history, color = 'b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch(0)\n",
      "Loss: 2.4897\n",
      "Accuracy on Validation Set: \n",
      "10.20%\n",
      "\n",
      "Loss: 2.0375\n",
      "Accuracy on Validation Set: \n",
      "25.90%\n",
      "\n",
      "Loss: 1.9039\n",
      "Accuracy on Validation Set: \n",
      "29.20%\n",
      "\n",
      "Loss: 1.6983\n",
      "Accuracy on Validation Set: \n",
      "39.40%\n",
      "\n",
      "Epoch(1)\n",
      "Loss: 1.7616\n",
      "Accuracy on Validation Set: \n",
      "43.30%\n",
      "\n",
      "Loss: 1.5410\n",
      "Accuracy on Validation Set: \n",
      "38.40%\n",
      "\n",
      "Loss: 1.3096\n",
      "Accuracy on Validation Set: \n",
      "44.30%\n",
      "\n",
      "Loss: 1.3703\n",
      "Accuracy on Validation Set: \n",
      "48.80%\n",
      "\n",
      "Epoch(2)\n",
      "Loss: 1.3998\n",
      "Accuracy on Validation Set: \n",
      "50.70%\n",
      "\n",
      "Loss: 1.2419\n",
      "Accuracy on Validation Set: \n",
      "51.60%\n",
      "\n",
      "Loss: 1.2316\n",
      "Accuracy on Validation Set: \n",
      "55.70%\n",
      "\n",
      "Loss: 1.1235\n",
      "Accuracy on Validation Set: \n",
      "55.70%\n",
      "\n",
      "Epoch(3)\n",
      "Loss: 1.1391\n",
      "Accuracy on Validation Set: \n",
      "60.30%\n",
      "\n",
      "Loss: 1.1330\n",
      "Accuracy on Validation Set: \n",
      "60.00%\n",
      "\n",
      "Loss: 1.2360\n",
      "Accuracy on Validation Set: \n",
      "61.60%\n",
      "\n",
      "Loss: 0.9398\n",
      "Accuracy on Validation Set: \n",
      "61.90%\n",
      "\n",
      "Epoch(4)\n",
      "Loss: 0.8993\n",
      "Accuracy on Validation Set: \n",
      "63.20%\n",
      "\n",
      "Loss: 0.9707\n",
      "Accuracy on Validation Set: \n",
      "64.50%\n",
      "\n",
      "Loss: 0.9632\n",
      "Accuracy on Validation Set: \n",
      "60.40%\n",
      "\n",
      "Loss: 0.9167\n",
      "Accuracy on Validation Set: \n",
      "67.20%\n",
      "\n",
      "Epoch(5)\n",
      "Loss: 0.8082\n",
      "Accuracy on Validation Set: \n",
      "62.20%\n",
      "\n",
      "Loss: 0.8827\n",
      "Accuracy on Validation Set: \n",
      "66.20%\n",
      "\n",
      "Loss: 0.6853\n",
      "Accuracy on Validation Set: \n",
      "67.60%\n",
      "\n",
      "Loss: 0.8209\n",
      "Accuracy on Validation Set: \n",
      "66.50%\n",
      "\n",
      "Epoch(6)\n",
      "Loss: 0.5407\n",
      "Accuracy on Validation Set: \n",
      "70.40%\n",
      "\n",
      "Loss: 0.7307\n",
      "Accuracy on Validation Set: \n",
      "70.30%\n",
      "\n",
      "Loss: 0.7630\n",
      "Accuracy on Validation Set: \n",
      "71.50%\n",
      "\n",
      "Loss: 0.7308\n",
      "Accuracy on Validation Set: \n",
      "71.50%\n",
      "\n",
      "Epoch(7)\n",
      "Loss: 0.8477\n",
      "Accuracy on Validation Set: \n",
      "72.60%\n",
      "\n",
      "Loss: 0.7007\n",
      "Accuracy on Validation Set: \n",
      "72.60%\n",
      "\n",
      "Loss: 0.6501\n",
      "Accuracy on Validation Set: \n",
      "76.90%\n",
      "\n",
      "Loss: 0.6432\n",
      "Accuracy on Validation Set: \n",
      "77.00%\n",
      "\n",
      "Epoch(8)\n",
      "Loss: 0.4965\n",
      "Accuracy on Validation Set: \n",
      "74.10%\n",
      "\n",
      "Loss: 0.6269\n",
      "Accuracy on Validation Set: \n",
      "75.40%\n",
      "\n",
      "Loss: 0.5489\n",
      "Accuracy on Validation Set: \n",
      "76.50%\n",
      "\n",
      "Loss: 0.7583\n",
      "Accuracy on Validation Set: \n",
      "77.10%\n",
      "\n",
      "Epoch(9)\n",
      "Loss: 0.4990\n",
      "Accuracy on Validation Set: \n",
      "77.30%\n",
      "\n",
      "Loss: 0.6086\n",
      "Accuracy on Validation Set: \n",
      "78.00%\n",
      "\n",
      "Loss: 0.4359\n",
      "Accuracy on Validation Set: \n",
      "77.90%\n",
      "\n",
      "Loss: 0.5550\n",
      "Accuracy on Validation Set: \n",
      "78.10%\n",
      "\n",
      "Epoch(10)\n",
      "Loss: 0.4734\n",
      "Accuracy on Validation Set: \n",
      "77.70%\n",
      "\n",
      "Loss: 0.4883\n",
      "Accuracy on Validation Set: \n",
      "79.50%\n",
      "\n",
      "Loss: 0.4364\n",
      "Accuracy on Validation Set: \n",
      "81.10%\n",
      "\n",
      "Loss: 0.5232\n",
      "Accuracy on Validation Set: \n",
      "79.60%\n",
      "\n",
      "Epoch(11)\n",
      "Loss: 0.4366\n",
      "Accuracy on Validation Set: \n",
      "80.80%\n",
      "\n",
      "Loss: 0.4795\n",
      "Accuracy on Validation Set: \n",
      "81.30%\n",
      "\n",
      "Loss: 0.4646\n",
      "Accuracy on Validation Set: \n",
      "83.40%\n",
      "\n",
      "Loss: 0.4714\n",
      "Accuracy on Validation Set: \n",
      "80.30%\n",
      "\n",
      "Epoch(12)\n",
      "Loss: 0.3461\n",
      "Accuracy on Validation Set: \n",
      "80.10%\n",
      "\n",
      "Loss: 0.3279\n",
      "Accuracy on Validation Set: \n",
      "81.00%\n",
      "\n",
      "Loss: 0.3492\n",
      "Accuracy on Validation Set: \n",
      "82.80%\n",
      "\n",
      "Loss: 0.4337\n",
      "Accuracy on Validation Set: \n",
      "83.20%\n",
      "\n",
      "Epoch(13)\n",
      "Loss: 0.3940\n",
      "Accuracy on Validation Set: \n",
      "82.50%\n",
      "\n",
      "Loss: 0.4122\n",
      "Accuracy on Validation Set: \n",
      "82.40%\n",
      "\n",
      "Loss: 0.4884\n",
      "Accuracy on Validation Set: \n",
      "80.50%\n",
      "\n",
      "Loss: 0.4415\n",
      "Accuracy on Validation Set: \n",
      "82.90%\n",
      "\n",
      "Epoch(14)\n",
      "Loss: 0.3078\n",
      "Accuracy on Validation Set: \n",
      "83.10%\n",
      "\n",
      "Loss: 0.4108\n",
      "Accuracy on Validation Set: \n",
      "82.60%\n",
      "\n",
      "Loss: 0.3251\n",
      "Accuracy on Validation Set: \n",
      "81.60%\n",
      "\n",
      "Loss: 0.3980\n",
      "Accuracy on Validation Set: \n",
      "84.60%\n",
      "\n",
      "Epoch(15)\n",
      "Loss: 0.4031\n",
      "Accuracy on Validation Set: \n",
      "80.50%\n",
      "\n",
      "Loss: 0.3524\n",
      "Accuracy on Validation Set: \n",
      "84.40%\n",
      "\n",
      "Loss: 0.3565\n",
      "Accuracy on Validation Set: \n",
      "84.60%\n",
      "\n",
      "Loss: 0.3310\n",
      "Accuracy on Validation Set: \n",
      "82.80%\n",
      "\n",
      "Epoch(16)\n",
      "Loss: 0.2856\n",
      "Accuracy on Validation Set: \n",
      "85.10%\n",
      "\n",
      "Loss: 0.4726\n",
      "Accuracy on Validation Set: \n",
      "83.50%\n",
      "\n",
      "Loss: 0.2919\n",
      "Accuracy on Validation Set: \n",
      "82.70%\n",
      "\n",
      "Loss: 0.2906\n",
      "Accuracy on Validation Set: \n",
      "83.60%\n",
      "\n",
      "Epoch(17)\n",
      "Loss: 0.1642\n",
      "Accuracy on Validation Set: \n",
      "85.70%\n",
      "\n",
      "Loss: 0.2038\n",
      "Accuracy on Validation Set: \n",
      "84.90%\n",
      "\n",
      "Loss: 0.2317\n",
      "Accuracy on Validation Set: \n",
      "85.60%\n",
      "\n",
      "Loss: 0.4406\n",
      "Accuracy on Validation Set: \n",
      "85.50%\n",
      "\n",
      "Epoch(18)\n",
      "Loss: 0.1994\n",
      "Accuracy on Validation Set: \n",
      "86.20%\n",
      "\n",
      "Loss: 0.2241\n",
      "Accuracy on Validation Set: \n",
      "86.10%\n",
      "\n",
      "Loss: 0.3137\n",
      "Accuracy on Validation Set: \n",
      "83.10%\n",
      "\n",
      "Loss: 0.4196\n",
      "Accuracy on Validation Set: \n",
      "85.00%\n",
      "\n",
      "Epoch(19)\n",
      "Loss: 0.3039\n",
      "Accuracy on Validation Set: \n",
      "85.50%\n",
      "\n",
      "Loss: 0.2735\n",
      "Accuracy on Validation Set: \n",
      "87.10%\n",
      "\n",
      "Loss: 0.3152\n",
      "Accuracy on Validation Set: \n",
      "84.50%\n",
      "\n",
      "Loss: 0.2871\n",
      "Accuracy on Validation Set: \n",
      "85.70%\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m Train(model, optimizer, epochs \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m Draw()\n",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36mDraw\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m m \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(accuracy_val_history) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m k \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(accuracy_train_history) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m----> 7\u001b[0m plt\u001b[39m.\u001b[39;49mplot(n, loss_history, color \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mg\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m plt\u001b[39m.\u001b[39mplot(m, accuracy_val_history, color \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[39m.\u001b[39mplot(k, accuracy_train_history, color \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2813\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2814\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/axes/_base.py:494\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(xy) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    493\u001b[0m     x \u001b[39m=\u001b[39m _check_1d(xy[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 494\u001b[0m     y \u001b[39m=\u001b[39m _check_1d(xy[\u001b[39m1\u001b[39;49m])\n\u001b[1;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     x, y \u001b[39m=\u001b[39m index_of(xy[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:1348\u001b[0m, in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[39m# plot requires `shape` and `ndim`.  If passed an\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[39m# object that doesn't provide them, then force to numpy array.\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m \u001b[39m# Note this will strip unit information.\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m'\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m         \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m'\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m         \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mshape) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m-> 1348\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49matleast_1d(x)\n\u001b[1;32m   1349\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/numpy/core/shape_base.py:65\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m ary \u001b[39min\u001b[39;00m arys:\n\u001b[0;32m---> 65\u001b[0m     ary \u001b[39m=\u001b[39m asanyarray(ary)\n\u001b[1;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m ary\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     67\u001b[0m         result \u001b[39m=\u001b[39m ary\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    969\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    971\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "model = MyResNet(BasicBlock, [3,4,6,3], 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                    momentum=0.9, weight_decay=1e-4)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "Train(model, optimizer, epochs = 20)\n",
    "Draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
